{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: r/drizzy and r/kanye subreddit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will be utilizing NLP preprocessing techniques in conjunction with classification metrics to classify subreddit comments from the r/drizzy and r/kanye subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "1. [EDA](#EDA)\n",
    "2. [Preprocessing](#Preprocessing)\n",
    "3. [Modeling](#Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drizzy = pd.read_csv('drizzy_comments.csv')\n",
    "kanye = pd.read_csv('kanye_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21200, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Lil Baby LOVES this line lmao</td>\n",
       "      <td>rmoler65</td>\n",
       "      <td>1626734500</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I'm not as huge a fan of KSG as some of you, b...</td>\n",
       "      <td>WhenItsHalfPastFive</td>\n",
       "      <td>1626734296</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Important to note also drizz and Justin were n...</td>\n",
       "      <td>therorax12</td>\n",
       "      <td>1626734151</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Donde Don Ye</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734135</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Maybe ye will have a song with drake titled re...</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734112</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               body  \\\n",
       "0         0.0                      Lil Baby LOVES this line lmao   \n",
       "1         1.0  I'm not as huge a fan of KSG as some of you, b...   \n",
       "2         2.0  Important to note also drizz and Justin were n...   \n",
       "3         3.0                                       Donde Don Ye   \n",
       "4         4.0  Maybe ye will have a song with drake titled re...   \n",
       "\n",
       "                author  created_utc subreddit  \n",
       "0             rmoler65   1626734500    Drizzy  \n",
       "1  WhenItsHalfPastFive   1626734296    Drizzy  \n",
       "2           therorax12   1626734151    Drizzy  \n",
       "3           roywoodsir   1626734135    Drizzy  \n",
       "4           roywoodsir   1626734112    Drizzy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(drizzy.shape)\n",
    "drizzy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22300, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1 is pretty cool but i prefer 2 it‚Äôs just so b...</td>\n",
       "      <td>Jtemelsooo</td>\n",
       "      <td>1626805100</td>\n",
       "      <td>Kanye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yeah you definitely shouldn't ü•¥</td>\n",
       "      <td>Royal_Seaworthiness3</td>\n",
       "      <td>1626805096</td>\n",
       "      <td>Kanye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Very possible</td>\n",
       "      <td>Sky_Screamer18</td>\n",
       "      <td>1626805076</td>\n",
       "      <td>Kanye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1626805051</td>\n",
       "      <td>Kanye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yeah u getting blocked lmao</td>\n",
       "      <td>Plainejanejohnnydang</td>\n",
       "      <td>1626805045</td>\n",
       "      <td>Kanye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0         0.0           0.0   \n",
       "1         1.0           1.0   \n",
       "2         2.0           2.0   \n",
       "3         3.0           3.0   \n",
       "4         4.0           4.0   \n",
       "\n",
       "                                                body                author  \\\n",
       "0  1 is pretty cool but i prefer 2 it‚Äôs just so b...            Jtemelsooo   \n",
       "1                    Yeah you definitely shouldn't ü•¥  Royal_Seaworthiness3   \n",
       "2                                      Very possible        Sky_Screamer18   \n",
       "3                                          [removed]             [deleted]   \n",
       "4                        Yeah u getting blocked lmao  Plainejanejohnnydang   \n",
       "\n",
       "   created_utc subreddit  \n",
       "0   1626805100     Kanye  \n",
       "1   1626805096     Kanye  \n",
       "2   1626805076     Kanye  \n",
       "3   1626805051     Kanye  \n",
       "4   1626805045     Kanye  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(kanye.shape)\n",
    "kanye.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drizzy.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "kanye.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21200 entries, 0 to 21199\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   body         21200 non-null  object\n",
      " 1   author       21200 non-null  object\n",
      " 2   created_utc  21200 non-null  int64 \n",
      " 3   subreddit    21200 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 662.6+ KB\n"
     ]
    }
   ],
   "source": [
    "drizzy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22300 entries, 0 to 22299\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   body         22300 non-null  object\n",
      " 1   author       22300 non-null  object\n",
      " 2   created_utc  22300 non-null  int64 \n",
      " 3   subreddit    22300 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 697.0+ KB\n"
     ]
    }
   ],
   "source": [
    "kanye.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([drizzy,kanye])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lil Baby LOVES this line lmao</td>\n",
       "      <td>rmoler65</td>\n",
       "      <td>1626734500</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm not as huge a fan of KSG as some of you, b...</td>\n",
       "      <td>WhenItsHalfPastFive</td>\n",
       "      <td>1626734296</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Important to note also drizz and Justin were n...</td>\n",
       "      <td>therorax12</td>\n",
       "      <td>1626734151</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donde Don Ye</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734135</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe ye will have a song with drake titled re...</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734112</td>\n",
       "      <td>Drizzy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body               author  \\\n",
       "0                      Lil Baby LOVES this line lmao             rmoler65   \n",
       "1  I'm not as huge a fan of KSG as some of you, b...  WhenItsHalfPastFive   \n",
       "2  Important to note also drizz and Justin were n...           therorax12   \n",
       "3                                       Donde Don Ye           roywoodsir   \n",
       "4  Maybe ye will have a song with drake titled re...           roywoodsir   \n",
       "\n",
       "   created_utc subreddit  \n",
       "0   1626734500    Drizzy  \n",
       "1   1626734296    Drizzy  \n",
       "2   1626734151    Drizzy  \n",
       "3   1626734135    Drizzy  \n",
       "4   1626734112    Drizzy  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43500, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1457\n",
       "[removed]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            670\n",
       "Yes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   60\n",
       "No                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    53\n",
       "u/savevideo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           52\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ... \n",
       "Wait people actually like drake üò≥                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
       "that‚Äôs a mountain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
       "SORRY BRO, you wanna message the moderators and let em know too. \\n\\nWad-else !                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "Views because he fully dove into different genres instead of taking different aspects from genres he likes. On Take Care he had dancehall influence on the song Take Care but on Views he actually jumped on multiple beats that were closer to that culture. \\n\\nHe also had songs where he jumped on southern and midwest beats of that time as well when he usually preferred east coast stuff when he wanted to flex his rapping prior to 2014. More Life is definitely his most ‚Äúout there‚Äù work tho as far as experimentation goes.                                                                              1\n",
       "I have no problem with this take, however it does seem very hypocritical to say that you ‚Äòdon‚Äôt respect people that shit on people on conditions they have no control over‚Äô but yet Drake has done this exact thing and you still respect him. So you can overlook it in some instances where the person has a better discography in your opinion?\\n\\nAlso, like you I am a fan of both Pusha and Drake. Both incredible artists who have affected the culture in huge ways. As I said, I agree with your take here, but I also find it hypocritical based on your earlier comment.\\n\\nAnyways, have a good day!       1\n",
       "Name: body, Length: 38804, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['body']!='[deleted]')&(df['body']!='[removed]')&(df['body']!='Yes')&(df['body']!='No')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "[Back to top](#Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using regex tokenizer to create word tokens in each reddit comment\n",
    "def tokens(body):\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    return tokenizer.tokenize(body.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['body'].apply(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lil Baby LOVES this line lmao</td>\n",
       "      <td>rmoler65</td>\n",
       "      <td>1626734500</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[lil, baby, loves, this, line, lmao]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm not as huge a fan of KSG as some of you, b...</td>\n",
       "      <td>WhenItsHalfPastFive</td>\n",
       "      <td>1626734296</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[i, m, not, as, huge, a, fan, of, ksg, as, som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Important to note also drizz and Justin were n...</td>\n",
       "      <td>therorax12</td>\n",
       "      <td>1626734151</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[important, to, note, also, drizz, and, justin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donde Don Ye</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734135</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[donde, don, ye]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe ye will have a song with drake titled re...</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734112</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[maybe, ye, will, have, a, song, with, drake, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body               author  \\\n",
       "0                      Lil Baby LOVES this line lmao             rmoler65   \n",
       "1  I'm not as huge a fan of KSG as some of you, b...  WhenItsHalfPastFive   \n",
       "2  Important to note also drizz and Justin were n...           therorax12   \n",
       "3                                       Donde Don Ye           roywoodsir   \n",
       "4  Maybe ye will have a song with drake titled re...           roywoodsir   \n",
       "\n",
       "   created_utc subreddit                                             tokens  \n",
       "0   1626734500    Drizzy               [lil, baby, loves, this, line, lmao]  \n",
       "1   1626734296    Drizzy  [i, m, not, as, huge, a, fan, of, ksg, as, som...  \n",
       "2   1626734151    Drizzy  [important, to, note, also, drizz, and, justin...  \n",
       "3   1626734135    Drizzy                                   [donde, don, ye]  \n",
       "4   1626734112    Drizzy  [maybe, ye, will, have, a, song, with, drake, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing the tokens\n",
    "def lemmas(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ', '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmas'] = df['tokens'].apply(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lil Baby LOVES this line lmao</td>\n",
       "      <td>rmoler65</td>\n",
       "      <td>1626734500</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[lil, baby, loves, this, line, lmao]</td>\n",
       "      <td>lil, baby, love, this, line, lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm not as huge a fan of KSG as some of you, b...</td>\n",
       "      <td>WhenItsHalfPastFive</td>\n",
       "      <td>1626734296</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[i, m, not, as, huge, a, fan, of, ksg, as, som...</td>\n",
       "      <td>i, m, not, a, huge, a, fan, of, ksg, a, some, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Important to note also drizz and Justin were n...</td>\n",
       "      <td>therorax12</td>\n",
       "      <td>1626734151</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[important, to, note, also, drizz, and, justin...</td>\n",
       "      <td>important, to, note, also, drizz, and, justin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donde Don Ye</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734135</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[donde, don, ye]</td>\n",
       "      <td>donde, don, ye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe ye will have a song with drake titled re...</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734112</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[maybe, ye, will, have, a, song, with, drake, ...</td>\n",
       "      <td>maybe, ye, will, have, a, song, with, drake, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body               author  \\\n",
       "0                      Lil Baby LOVES this line lmao             rmoler65   \n",
       "1  I'm not as huge a fan of KSG as some of you, b...  WhenItsHalfPastFive   \n",
       "2  Important to note also drizz and Justin were n...           therorax12   \n",
       "3                                       Donde Don Ye           roywoodsir   \n",
       "4  Maybe ye will have a song with drake titled re...           roywoodsir   \n",
       "\n",
       "   created_utc subreddit                                             tokens  \\\n",
       "0   1626734500    Drizzy               [lil, baby, loves, this, line, lmao]   \n",
       "1   1626734296    Drizzy  [i, m, not, as, huge, a, fan, of, ksg, as, som...   \n",
       "2   1626734151    Drizzy  [important, to, note, also, drizz, and, justin...   \n",
       "3   1626734135    Drizzy                                   [donde, don, ye]   \n",
       "4   1626734112    Drizzy  [maybe, ye, will, have, a, song, with, drake, ...   \n",
       "\n",
       "                                              lemmas  \n",
       "0                  lil, baby, love, this, line, lmao  \n",
       "1  i, m, not, a, huge, a, fan, of, ksg, a, some, ...  \n",
       "2  important, to, note, also, drizz, and, justin,...  \n",
       "3                                     donde, don, ye  \n",
       "4  maybe, ye, will, have, a, song, with, drake, t...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                503\n",
       "lmao             65\n",
       "u, savevideo     59\n",
       "fact             49\n",
       "w                45\n",
       "nah              43\n",
       "yes              43\n",
       "what             40\n",
       "no               39\n",
       "thank, you       39\n",
       "Name: lemmas, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmas'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>üíØ</td>\n",
       "      <td>thebiggershort</td>\n",
       "      <td>1626724874</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>üó£üíØ</td>\n",
       "      <td>studyingdeath</td>\n",
       "      <td>1626692782</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>ü§ù</td>\n",
       "      <td>trapavellited2</td>\n",
       "      <td>1626682698</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>üò≥üòÅüòï</td>\n",
       "      <td>canadav2</td>\n",
       "      <td>1626663811</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>ü§¢</td>\n",
       "      <td>poorlyparkedcars</td>\n",
       "      <td>1626663749</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    body            author  created_utc subreddit tokens lemmas\n",
       "130    üíØ    thebiggershort   1626724874    Drizzy     []       \n",
       "450   üó£üíØ     studyingdeath   1626692782    Drizzy     []       \n",
       "480    ü§ù    trapavellited2   1626682698    Drizzy     []       \n",
       "661  üò≥üòÅüòï          canadav2   1626663811    Drizzy     []       \n",
       "665    ü§¢  poorlyparkedcars   1626663749    Drizzy     []       "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to remove non-text lemmas\n",
    "df[df['lemmas'] == ''].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lemmas = df['lemmas'].value_counts()[:10].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all lemmas that are either emojis or stop words\n",
    "df = df[df['lemmas'].isin(bad_lemmas) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check whether or not lemmas have numbers in them, returns list of lemmas without numbers\n",
    "def num_check(tokens):\n",
    "    non_alnum = []\n",
    "    alnum = []\n",
    "    for token in tokens:\n",
    "        for char in str(token):\n",
    "            if str(char) in '1234567890':\n",
    "                alnum.append(token)\n",
    "    for token in tokens:\n",
    "        if token not in alnum:\n",
    "            non_alnum.append(token)\n",
    "    return non_alnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(num_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check if there are any words that contain more than three consecutive letters, returns only words\n",
    "#without three consecutive letters\n",
    "def check_non_words(tokens):\n",
    "    real_words = []\n",
    "    for token in tokens:\n",
    "        count = 0\n",
    "        previous_char = 0\n",
    "        for char in token:\n",
    "            if char == previous_char:\n",
    "                count += 1\n",
    "            previous_char = char\n",
    "        if count <= 2:\n",
    "            real_words.append(token)\n",
    "    return real_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "check_non_words(['hello','hellllllo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying function to remove words with improper spelling\n",
    "df['tokens'] = df['tokens'].apply(check_non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lil Baby LOVES this line lmao</td>\n",
       "      <td>rmoler65</td>\n",
       "      <td>1626734500</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[lil, baby, loves, this, line, lmao]</td>\n",
       "      <td>lil, baby, love, this, line, lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm not as huge a fan of KSG as some of you, b...</td>\n",
       "      <td>WhenItsHalfPastFive</td>\n",
       "      <td>1626734296</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[i, m, not, as, huge, a, fan, of, ksg, as, som...</td>\n",
       "      <td>i, m, not, a, huge, a, fan, of, ksg, a, some, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Important to note also drizz and Justin were n...</td>\n",
       "      <td>therorax12</td>\n",
       "      <td>1626734151</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[important, to, note, also, drizz, and, justin...</td>\n",
       "      <td>important, to, note, also, drizz, and, justin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donde Don Ye</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734135</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[donde, don, ye]</td>\n",
       "      <td>donde, don, ye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe ye will have a song with drake titled re...</td>\n",
       "      <td>roywoodsir</td>\n",
       "      <td>1626734112</td>\n",
       "      <td>Drizzy</td>\n",
       "      <td>[maybe, ye, will, have, a, song, with, drake, ...</td>\n",
       "      <td>maybe, ye, will, have, a, song, with, drake, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body               author  \\\n",
       "0                      Lil Baby LOVES this line lmao             rmoler65   \n",
       "1  I'm not as huge a fan of KSG as some of you, b...  WhenItsHalfPastFive   \n",
       "2  Important to note also drizz and Justin were n...           therorax12   \n",
       "3                                       Donde Don Ye           roywoodsir   \n",
       "4  Maybe ye will have a song with drake titled re...           roywoodsir   \n",
       "\n",
       "   created_utc subreddit                                             tokens  \\\n",
       "0   1626734500    Drizzy               [lil, baby, loves, this, line, lmao]   \n",
       "1   1626734296    Drizzy  [i, m, not, as, huge, a, fan, of, ksg, as, som...   \n",
       "2   1626734151    Drizzy  [important, to, note, also, drizz, and, justin...   \n",
       "3   1626734135    Drizzy                                   [donde, don, ye]   \n",
       "4   1626734112    Drizzy  [maybe, ye, will, have, a, song, with, drake, ...   \n",
       "\n",
       "                                              lemmas  \n",
       "0                  lil, baby, love, this, line, lmao  \n",
       "1  i, m, not, a, huge, a, fan, of, ksg, a, some, ...  \n",
       "2  important, to, note, also, drizz, and, justin,...  \n",
       "3                                     donde, don, ye  \n",
       "4  maybe, ye, will, have, a, song, with, drake, t...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the lemmas with updated tokens\n",
    "df['lemmas'] = df['tokens'].apply(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing value of subreddit column for classification\n",
    "df['subreddit'] = df['subreddit'].map({'Drizzy':1,'Kanye':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.512061\n",
       "1    0.487939\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline score for model comparison is 51%\n",
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Sections:\n",
    "1. [Count Vectorizer Models](#Using-CountVectorizer)\n",
    "2. [Tfidf Vectorizer Models](#Using-TfidfVectorizer)\n",
    "\n",
    "[Back to top](#Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up variables for models\n",
    "X = df['lemmas']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CountVectorizer\n",
    "1. [Random Forest](#Random-Forest-Model-(Cvec))\n",
    "2. [SVC](#SVC-Model-(Cvec))\n",
    "3. [AdaBoost](#AdaBoost-(Cvec))\n",
    "4. [Naive Bayes](#Naive-Bayes-(Cvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model (Cvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pipeline for cvec, rf test\n",
    "pipe = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),max_features=3000)),\n",
    "    ('rf',RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=3000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('rf', RandomForestClassifier())])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7178605708229362"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(pipe,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6954589855277759"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(pipe,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2 (RandomSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('cvec',\n",
       "                                              CountVectorizer(max_features=3000,\n",
       "                                                              stop_words=['i',\n",
       "                                                                          'me',\n",
       "                                                                          'my',\n",
       "                                                                          'myself',\n",
       "                                                                          'we',\n",
       "                                                                          'our',\n",
       "                                                                          'ours',\n",
       "                                                                          'ourselves',\n",
       "                                                                          'you',\n",
       "                                                                          \"you're\",\n",
       "                                                                          \"you've\",\n",
       "                                                                          \"you'll\",\n",
       "                                                                          \"you'd\",\n",
       "                                                                          'your',\n",
       "                                                                          'yours',\n",
       "                                                                          'yourself',\n",
       "                                                                          'yourselves',\n",
       "                                                                          'he',\n",
       "                                                                          'him',\n",
       "                                                                          'his',\n",
       "                                                                          'himself',\n",
       "                                                                          'she',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'her',\n",
       "                                                                          'hers',\n",
       "                                                                          'herself',\n",
       "                                                                          'it',\n",
       "                                                                          \"it's\",\n",
       "                                                                          'its',\n",
       "                                                                          'itself', ...])),\n",
       "                                             ('rf', RandomForestClassifier())]),\n",
       "                   param_distributions={'rf__max_depth': [None, 1, 2, 3, 4, 5],\n",
       "                                        'rf__n_estimators': [100, 150, 200]})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating pipe and hyperparam dict for random search of rf model\n",
    "pipe2 = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),max_features=3000)),\n",
    "    ('rf',RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'rf__n_estimators':[100,150,200],\n",
    "    'rf__max_depth':[None,1,2,3,4,5]\n",
    "}\n",
    "rs = RandomizedSearchCV(pipe2,param_distributions=params,cv=5)\n",
    "rs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7215629135816632"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best score from random search\n",
    "rs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__n_estimators': 200, 'rf__max_depth': None}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparams from the random search\n",
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['lemmas']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=1000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('rf', RandomForestClassifier(n_estimators=200))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating model with best hyperparams and a lower max_feature value\n",
    "simple_pipe2 = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),max_features=1000)),\n",
    "    ('rf',RandomForestClassifier(max_depth=None, n_estimators=200))\n",
    "])\n",
    "simple_pipe2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7035468708812559"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv for train data\n",
    "cross_val_score(simple_pipe2,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849477359114196"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv for test data\n",
    "cross_val_score(simple_pipe2,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Model (Cvec)\n",
    "[cvec](#Using-CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=1000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('svc', SVC())])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline for svc model with cvec, using only max_features of 1000 hyperparam from last iterations\n",
    "pipe_1_svc = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),max_features=1000)),\n",
    "    ('svc',SVC())\n",
    "])\n",
    "pipe_1_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7013321642075417"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(pipe_1_svc,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6791950956551849"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(pipe_1_svc,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=500,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('svc', SVC(C=10, kernel='poly'))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline with different hyperparams\n",
    "pipe_2_svc = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),max_features=500)),\n",
    "    ('svc',SVC(C=10,kernel='poly'))\n",
    "])\n",
    "pipe_2_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6532344875718923"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(pipe_2_svc,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.635068239409464"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(pipe_2_svc,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost (Cvec)\n",
    "[cvec](#Using-CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=1000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('ada', AdaBoostClassifier())])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline for AdaBoost model with minimal hyperparams\n",
    "pipe_boost = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),max_features=1000)),\n",
    "    ('ada',AdaBoostClassifier())\n",
    "])\n",
    "pipe_boost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6532676820032479"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for tain data\n",
    "cross_val_score(pipe_boost,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6469663613255582"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(pipe_boost,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (Cvec)\n",
    "[cvec](#Using-CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=1000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline for naive bayes with minimal hyperparams\n",
    "nb_pipe = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),max_features=1000)),\n",
    "    ('nb',MultinomialNB())\n",
    "])\n",
    "nb_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7045388413362452"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(nb_pipe,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7002184802197198"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(nb_pipe,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.8, max_features=1000, min_df=10,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline for nb with more cvec hyperparams\n",
    "nb_pipe2 = Pipeline([\n",
    "    ('cvec',CountVectorizer(stop_words=stopwords.words('english'),\n",
    "                            max_features=1000,\n",
    "                            max_df=0.8,\n",
    "                            min_df=10)),\n",
    "    ('nb',MultinomialNB())\n",
    "])\n",
    "nb_pipe2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7047041142028296"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(nb_pipe2,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7018048964751988"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(nb_pipe2,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viz for Cvec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#creating graph for cvec most common words\n",
    "cvec_df.sum().sort_values(ascending=False).head(10).plot(kind='barh')\n",
    "plt.title('Most Common CountVectorizer Words')\n",
    "plt.xlabel('Number of Appearances')\n",
    "plt.ylabel('Lemmas');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TfidfVectorizer\n",
    "[Modeling](#Modeling)\n",
    "1. [Random Forest](#Random-Forest-Model-(Tfidf))\n",
    "2. [SVC](#SVC-Model-(Tfidf))\n",
    "3. [AdaBoost](#AdaBoost-Model-(Tfidf))\n",
    "4. [Naive Bayes](#Naive-Bayes-Model-(Tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model (Tfidf)\n",
    "[Modeling](#Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=2000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('rf', RandomForestClassifier())])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline for random forest with tfidf\n",
    "rf_pipe = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(stop_words=stopwords.words('english'),max_features=2000)),\n",
    "    ('rf',RandomForestClassifier())\n",
    "])\n",
    "rf_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7179269050536009"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(rf_pipe,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6929799088698443"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(rf_pipe,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Model (Tfidf)\n",
    "[Modeling](#Using-TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=1000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('svc', SVC())])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline for svc model with tfidf preprocessing\n",
    "svc_pipe = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(stop_words=stopwords.words('english'),max_features=1000)),\n",
    "    ('svc',SVC())\n",
    "])\n",
    "svc_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7128030255227262"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(svc_pipe,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6966482320907208"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(svc_pipe,X_test,y_test,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Model (Tfidf)\n",
    "[Modeling](#Using-TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=1000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('ada', AdaBoostClassifier())])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline with minimal hyperparams for AdaBoost model with tfidf preprocessing\n",
    "pipe_boost_tf = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(stop_words=stopwords.words('english'),max_features=1000)),\n",
    "    ('ada',AdaBoostClassifier())\n",
    "])\n",
    "pipe_boost_tf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.651317394433814"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv for train data\n",
    "cross_val_score(pipe_boost_tf,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.651317394433814"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv for test data\n",
    "cross_val_score(pipe_boost_tf,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model (Tfidf)\n",
    "[Modeling](#Using-TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=1000,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline for naive bayes model with minimal hyperparams\n",
    "nb_pipe2 = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(stop_words=stopwords.words('english'),max_features=1000)),\n",
    "    ('nb',MultinomialNB())\n",
    "])\n",
    "nb_pipe2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7023238341862761"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for train data\n",
    "cross_val_score(nb_pipe2,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6971443130218539"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cv score for test data\n",
    "cross_val_score(nb_pipe2,X_test,y_test,cv=5).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
